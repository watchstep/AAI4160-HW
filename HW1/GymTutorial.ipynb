{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Familiar with Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gym` serves as an interface for RL experiments, providing standardized APIs to interact with a variety of environments. `MuJoCo`, on the other hand, provides the physics engine that underpins many of these environments.\n",
    "\n",
    "* `Gym` is a standardized API for reinforcement learning, offering a broad collection of reference environments. It's designed to be simple and Pythonic, allowing for the representation of general RL problems. Users can create environments, initiate them, execute actions based on a user-defined policy, and manage the environment's state, including resetting it upon termination or truncation of episodes. Here's the documentation you would find useful: [Gym Documentation](https://www.gymlibrary.dev/index.html).\n",
    "* `MuJoCo`, which stands for Multi-Joint dynamics with Contact, is a high-performance physics engine for simulating complex dynamic systems with a focus on robotics, biomechanics, and other areas requiring fast and accurate simulations. It is now open-sourced by DeepMind as of 2022, making it freely available. It includes various environments like `Ant`, `HalfCheetah`, `Hopper`, `Humanoid`, etc. You can see more in the [Gym Documentation on MuJoCo](https://www.gymlibrary.dev/environments/mujoco/index.html).\n",
    "\n",
    "In this tutorial, you'll get to know how to interact with `Gym`'s interface with the `MuJoCo` and some other environments. `Gym` is designed to be universal, making it easy to switch between different environments without changing your setup much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the code, we strongly recommend you check out `installation.md` first and follow the installation procedures. If followed correctly, the requirements listed below would have been already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mujoco==2.2.0\n",
    "!pip install gym==0.25.2\n",
    "!pip install tensorboard==2.10.0\n",
    "!pip install tensorboardX==2.5.1\n",
    "!pip install matplotlib==3.5.3\n",
    "!pip install ipython==7.34.0\n",
    "!pip install moviepy==1.0.3\n",
    "!pip install pyvirtualdisplay==3.0\n",
    "!pip install torch==1.13.1\n",
    "!pip install opencv-python==4.6.0.66\n",
    "!pip install ipdb==0.13.9\n",
    "!pip install swig==4.0.2\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install mediapy\n",
    "\n",
    "!pip install gym[classic_control,toytext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym is our gateway to various reinforcement learning environments, including MuJoCo.\n",
    "\n",
    "You can easily create a gym environment with `gym.make`. Here's how you can do this with `Ant-v4` environment from `MuJoCo`.\n",
    "\n",
    "Also, in the following code you may notice the usage of `env.reset` and `env.render`:\n",
    "- `env.reset()`: This method resets the environment to its initial state, and returns the observation of the environment corresponding to the initial state.\n",
    "- `env.render(mode=\"rgb_array\")`: This method renders the current state as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ant environment\n",
    "env_name = \"Ant-v4\"\n",
    "env = gym.make(env_name, terminate_when_unhealthy=False)\n",
    "print(f\"Initialized environment: {env_name}\")\n",
    "\n",
    "initial_obs = env.reset()\n",
    "rendered_state = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(rendered_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `Ant-v4` environment simulates a quadruped robot in a 2D space. The ant is a 3D robot consisting of one torso (free rotational body) with four legs attached to it with each leg having two links. The goal is to coordinate the four legs to move in the forward (right) direction by applying torques on the eight hinges connecting the two links of each leg and the torso (nine parts and eight hinges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment defines an attribute called the `observation_space` and `action_space`.\n",
    "You can see how it looks like below.\n",
    "The observation space of `Ant-v4` has 27 distinct elements, and the action space has 8 dimensions.\n",
    "If you want to figure out what each dimension of the observations and the actions means, it would be helpful to see the documents of the environment.\n",
    "[Ant Documentation](https://www.gymlibrary.dev/environments/mujoco/ant/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(f\"The observation space: {obs_space}\")\n",
    "print(f\"The action space: {action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously discussed, we can create many other environments with `gym.make`, with the general interface provided with `gym`.\n",
    "\n",
    "Here's some other examples, which is not actually from `MuJoCo`, but great for illustrating different observation and action spaces are possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_visualize_gym_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env.reset()\n",
    "    rendered_image = env.render(mode=\"rgb_array\")\n",
    "    plt.title(env_name)\n",
    "    plt.imshow(rendered_image)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"The observation space of {env_name}: {env.observation_space}\")\n",
    "    print(f\"The action space of {env_name}: {env.action_space}\")\n",
    "    return env\n",
    "\n",
    "create_and_visualize_gym_env(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CartPole-v1` has discrete action space, where the agent can choose to push the cart to the left or to the right.\n",
    "The goal of the agent is to keep the pole balanced on the cart for as long as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_visualize_gym_env(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FrozenLake-v1` environment has a 4x4 grid as an observation space, where each value represents different tiles in the grid.\n",
    "In this environment, the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "You can see that `FrozenLake-v1` has discrete observation space as well as discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with the environment, you can use the methods of the `Env` class. Here's the two important functions.\n",
    "- `reset`: This function resets the environment to its initial state, and returns the observation of the environment corresponding to the initial state.\n",
    "- `step` : This function takes an action as an input and applies it to the environment, which leads to the environment transitioning to a new state. The reset function returns four things:\n",
    "\n",
    "1. `observation`: The observation of the state of the environment.\n",
    "2. `reward`: The reward that you can get from the environment after executing the action that was given as the input to the step function.\n",
    "3. `done`: Whether the episode has been terminated. If true, you may need to end the simulation or reset the environment to restart the episode.\n",
    "4. `info`: This provides additional information depending on the environment, such as number of lives left, or general information that may be conducive in debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with a simple code that takes a single step of the environment with a random action. Note here that you can sample a random action from the action space using `env.action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Ant-v4\", terminate_when_unhealthy=False)\n",
    "\n",
    "# Reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# Take the action and get the new observation space\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_obs))\n",
    "print(\"The reward is {}\".format(reward))\n",
    "print(\"Is the environment terminated?: {}\".format(done))\n",
    "print(\"Additional informations: {}\".format(info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_image = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(rendered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you figured out how to take an environment step, let's collect and visualize trajectories from the environment with random policy.\n",
    "\n",
    "In reinforcement learning, an `episode` represents a sequence of steps taken by an agent from the initial state to a terminal state.\n",
    "We will now simulate the environment's behavior under a random policy, capturing each step as a frame. The run_episode function executes `frames_per_episodes` steps and return the observations (states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for our simulation\n",
    "frames_per_episode = 300  # Number of steps (frames) per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate an episode with a random policy\n",
    "def run_episode(env, frames_per_episode):\n",
    "    frames = []  # Collect frames for video rendering\n",
    "    env.reset()\n",
    "    for _ in range(frames_per_episode):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "# Visualize one episode with a random policy\n",
    "frames = run_episode(env, frames_per_episode)\n",
    "\n",
    "media.show_video(frames, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reset the environment using `env.reset()`. Note that the return value of the reset function is the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()\n",
    "print(\"initial observation: {}\\n\".format(ob))\n",
    "rendered_image = env.render(mode=\"rgb_array\")\n",
    "media.show_image(rendered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize random trajectories with other environments that we discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "ob = env.reset()\n",
    "\n",
    "# Visualize 10 episodes with a random policy\n",
    "whole_frames = []\n",
    "for _ in range(10):\n",
    "    frames = run_episode(env, frames_per_episode=100)\n",
    "    whole_frames.extend(frames)\n",
    "\n",
    "media.show_video(whole_frames, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "ob = env.reset()\n",
    "\n",
    "# Visualize 10 episodes with a random policy\n",
    "whole_frames = []\n",
    "for _ in range(10):\n",
    "    frames = run_episode(env, frames_per_episode=100)\n",
    "    whole_frames.extend(frames)\n",
    "\n",
    "media.show_video(whole_frames, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you learned how to create and visualize a gym environment, and how to simulate an episode with a random policy! Feel free to try some other environments that you can find from [gym documentation](https://www.gymlibrary.dev/environments/mujoco/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
